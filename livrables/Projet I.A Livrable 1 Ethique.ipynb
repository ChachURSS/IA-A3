{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c67fa6ef",
   "metadata": {},
   "source": [
    "# Projet I.A. – Livrable éthique  \n",
    "**HumanForYou – Prédiction du turnover**\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## Sommaire :\n",
    "\n",
    "\n",
    "\n",
    "1. Contexte du projet et démarche éthique  \n",
    "\n",
    "2. Respect de l’autonomie humaine  \n",
    "\n",
    "3. Robustesse technique et sécurité  \n",
    "\n",
    "4. Confidentialité et gouvernance des données  \n",
    "\n",
    "5. Transparence  \n",
    "\n",
    "6. Diversité, non-discrimination et équité  \n",
    "\n",
    "7. Bien-être environnemental et sociétal  \n",
    "\n",
    "8. Responsabilité  \n",
    "\n",
    "9. Conclusion    \n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Contexte du projet et démarche éthique\n",
    "\n",
    "HumanForYou est une entreprise pharmaceutique basée en Inde, avec environ 4000 employés et un taux de turnover annuel d’environ 15 %. <br>\n",
    "Ce niveau de rotation entraîne des coûts de recrutement et de formation, une instabilité dans les équipes et un impact sur la réputation de l’entreprise.\n",
    "\n",
    "Notre projet consiste à utiliser des techniques de machine learning pour **analyser les facteurs associés au départ des employés (Attrition)** et **prédire le risque de départ**, à partir de données RH (âge, salaire, poste, ancienneté, etc.), d’enquêtes de satisfaction et d’horaires de travail issus des badgeuses.\n",
    "\n",
    "Pour structurer notre réflexion, nous nous sommes appuyés sur l’**Assessment List for Trustworthy Artificial Intelligence (ALTAI)** proposée par le High-Level Expert Group on AI pour la Commission européenne. Cette liste opérationnalise les 7 exigences des *Ethics Guidelines for Trustworthy AI*, dans lesquelles une IA digne de confiance doit être à la fois *« lawful, ethical and robust »*.  \n",
    "\n",
    "Les lignes directrices identifient sept exigences clés : **human agency and oversight, technical robustness and safety, privacy and data governance, transparency, diversity/non-discrimination/fairness, societal and environmental well-being, accountability**. ALTAI traduit ces principes en une checklist pratique qui vise à *« ensure that users benefit from AI without being exposed to unnecessary risks »*.  \n",
    "\n",
    "Dans notre projet, nous avons utilisé ces exigences comme grille d’analyse pour examiner :\n",
    "- les données utilisées,  \n",
    "- les choix de modélisation,  \n",
    "- et surtout les usages possibles (et les usages à éviter) du système par HumanForYou.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Respect de l’autonomie humaine\n",
    "\n",
    "Le premier principe mis en avant par la Commission est **“human agency and oversight”** : les systèmes d’IA doivent *« empower human beings, allowing them to make informed decisions »* et rester sous contrôle humain.\n",
    "\n",
    "Dans ce projet, nous avons donc explicitement posé que le modèle de prédiction du turnover est un **outil d’aide à la décision** pour les RH, et non un outil de décision automatique. L’objectif est d’identifier les situations où un employé semble à risque de départ afin de :\n",
    "\n",
    "- détecter des cas de surcharge, de démotivation ou de déséquilibre vie pro/vie perso ;  \n",
    "- prioriser des actions positives : entretiens, formations, mobilité interne, révision de certaines conditions de travail.\n",
    "\n",
    "Nous avons également identifié des usages à proscrire : utilisation du score pour justifier un refus de promotion, pour cibler des licenciements ou pour écarter des candidats jugés “à risque” au recrutement.  \n",
    "\n",
    "Dans une perspective de déploiement réel, il serait nécessaire de prévoir des **mécanismes de contestation** : un employé ne devrait pas être pénalisé sans explication ni possibilité de faire valoir sa situation.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Robustesse technique et sécurité\n",
    "\n",
    "La deuxième exigence est **“technical robustness and safety”** : les systèmes d’IA doivent être *« resilient and secure »*, avec des résultats fiables et reproductibles pour minimiser les dommages involontaires.\n",
    "\n",
    "Concrètement, nous avons cherché à garantir cette robustesse par plusieurs décisions :\n",
    "\n",
    "- Séparation des données en **jeu d’entraînement** et **jeu de test** (stratifié sur la variable Attrition), afin d’évaluer le modèle sur des données non vues.  \n",
    "- Comparaison de plusieurs modèles (régression logistique, arbres, random forests, éventuellement gradient boosting) pour éviter de se reposer sur un seul algorithme.  \n",
    "- Prise en compte du **déséquilibre de classes** (peu de “Attrition = Yes”) via des pondérations de classe ou du rééchantillonnage.  \n",
    "- Utilisation de métriques adaptées : recall et précision sur la classe “départ”, F1-score et AUC, plutôt que l’accuracy seule.\n",
    "\n",
    "Nous avons aussi discuté des limites : les données datent de 2015 et viennent d’un contexte particulier (pharma, Inde). Dans un déploiement réel, il faudrait **réévaluer périodiquement** le modèle (dérive des données, nouveaux modes de travail, etc.) et prévoir des plans de repli si ses performances se dégradent.\n",
    "\n",
    "Sur la sécurité, nous considérons que l’accès aux données et au modèle doit être restreint, avec un stockage sécurisé et une traçabilité des accès.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Confidentialité et gouvernance des données\n",
    "\n",
    "La troisième exigence européenne est **“privacy and data governance”** : au-delà du respect de la vie privée, il s’agit d’assurer une bonne qualité des données et un accès légitime à celles-ci.\n",
    "\n",
    "Dans notre cas, les fichiers fournis sont déjà **anonymisés** : chaque employé est identifié par un EmployeeID qui ne permet pas, dans le cadre pédagogique du projet, de remonter à une personne réelle. Nous avons néanmoins appliqué un principe de **minimisation** :\n",
    "\n",
    "- éviter de multiplier les variables inutiles ;  \n",
    "- ne pas croiser avec des données externes (réseaux sociaux, santé, etc.) ;  \n",
    "- traiter les réponses aux enquêtes de satisfaction avec prudence (le fait de ne pas répondre peut avoir plusieurs explications).\n",
    "\n",
    "Si le système était déployé, une gouvernance claire devrait être définie :\n",
    "\n",
    "- qui peut consulter les données détaillées, qui peut voir uniquement les prédictions agrégées ;  \n",
    "- combien de temps les données et les modèles sont conservés ;  \n",
    "- comment les transformations (nettoyage, encodage, agrégation des horaires de badge) sont documentées dans un registre.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Transparence\n",
    "\n",
    "La quatrième exigence, **“transparency”**, recommande que *« the data, system and AI business models should be transparent »* et que les décisions des systèmes d’IA soient expliquées de manière adaptée aux parties prenantes.\n",
    "\n",
    "Dans notre projet, nous avons cherché à rester transparents à plusieurs niveaux :\n",
    "\n",
    "- Sur le choix des modèles : au minimum une régression logistique et des arbres de décision, afin de pouvoir expliquer quels facteurs augmentent ou diminuent la probabilité de départ.  \n",
    "- Sur les **variables importantes** : par exemple, l’ancienneté, la satisfaction au travail, l’équilibre vie pro/vie perso ou la distance domicile–travail peuvent ressortir comme signaux forts. Nous les présentons explicitement dans notre analyse.  \n",
    "- Sur la nature même du score : ce n’est pas un verdict, mais une estimation probabiliste basée sur des données historiques, avec une part d’incertitude.\n",
    "\n",
    "La transparence implique aussi une **communication adaptée** aux différents publics : direction, RH, managers, et idéalement employés. Tous doivent comprendre ce que le système fait, les types de données utilisées et, tout aussi important, ce qu’il ne fait pas (pas d’accès à la vie privée extérieure, pas de lecture des emails, etc.).\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Diversité, non-discrimination et équité\n",
    "\n",
    "La cinquième exigence est **“diversity, non-discrimination and fairness”**. Les lignes directrices insistent sur le fait que *« unfair bias must be avoided »*, car il peut marginaliser des groupes vulnérables ou renforcer des discriminations existantes.\n",
    "\n",
    "Dans le cas de HumanForYou, certaines variables touchent à des caractéristiques sensibles ou quasi-sensibles : genre, âge, statut marital, niveau d’étude, type de poste. Il est possible que l’histoire de l’entreprise montre des taux de départ différents entre groupes pour des raisons structurelles (horaires, culture managériale, opportunités de mobilité, etc.).\n",
    "\n",
    "Notre démarche a été la suivante :\n",
    "\n",
    "- Utiliser ces variables pour **analyser les biais** du modèle et les éventuelles inégalités dans l’organisation (par exemple, vérifier si le modèle se trompe plus souvent pour certains groupes).  \n",
    "- Refuser l’idée que ces variables puissent servir à **justifier des décisions défavorables** pour un groupe donné (“le modèle dit que ce type de profil part souvent, donc on investit moins sur eux”). Au contraire, elles doivent alerter sur des problèmes à corriger.\n",
    "\n",
    "Nous recommandons qu’un déploiement réel s’accompagne d’un **audit régulier** des performances par sous-groupe (genre, âge, type de poste, etc.) et, idéalement, d’un dialogue avec des représentants des salariés pour discuter de l’impact du système.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Bien-être environnemental et sociétal\n",
    "\n",
    "La sixième exigence est **“societal and environmental well-being”** : les systèmes d’IA devraient *« benefit all human beings, including future generations »* et être durables, en prenant en compte leur impact sur l’environnement et la société.\n",
    "\n",
    "Dans notre projet :\n",
    "\n",
    "- L’impact environnemental direct reste limité (dataset modeste, modèles relativement simples), et nous avons justement privilégié des approches de type régression ou arbres plutôt que des architectures de deep learning coûteuses en calcul pour un problème qui ne l’exige pas.  \n",
    "- Sur le plan sociétal, l’enjeu est important : un bon usage du modèle peut contribuer à améliorer la qualité de vie au travail (meilleure détection des situations de risque, actions de prévention…) et à stabiliser les équipes, ce qui bénéficie à l’entreprise mais aussi aux employés et, indirectement, aux patients et partenaires.\n",
    "\n",
    "Le risque serait qu’un tel outil soit utilisé uniquement pour optimiser un indicateur de turnover au détriment du bien-être réel : par exemple, en cherchant à retenir coûte que coûte des employés sans traiter les causes profondes (charge de travail, reconnaissance, possibilités de progression). L’utilisation de l’outil doit donc être clairement orientée vers l’amélioration des **conditions de travail**, pas seulement vers une réduction chiffrée du taux de départ.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Responsabilité\n",
    "\n",
    "Enfin, la septième exigence porte sur **“accountability”** : des mécanismes doivent assurer la responsabilité et la traçabilité des systèmes d’IA et de leurs résultats, avec la possibilité d’audit et de recours.\n",
    "\n",
    "Dans notre projet, nous distinguons plusieurs niveaux de responsabilité :\n",
    "\n",
    "- Les **data scientists / analystes** sont responsables de la qualité de la modélisation : documentation des données, description des méthodes, analyse des limites et des biais possibles.  \n",
    "- Le **service RH** et la **direction** sont responsables de la façon dont le modèle est intégré dans les processus décisionnels : formation des utilisateurs, définition des usages autorisés, mise en place de procédures de validation.  \n",
    "- L’**organisation** dans son ensemble est responsable de la mise en place de mécanismes de contrôle : audit régulier du système, procédures de révision du modèle, possibilité pour les employés de poser des questions et de contester des décisions.\n",
    "\n",
    "En pratique, un déploiement réel devrait s’accompagner :\n",
    "- d’une **charte d’utilisation** du modèle, détaillant les finalités légitimes et les usages interdits ;  \n",
    "- de procédures internes permettant un **recours effectif** pour les employés, avec une personne ou une instance clairement identifiée comme responsable.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Conclusion\n",
    "\n",
    "En nous appuyant sur l’ALTAI et les lignes directrices européennes, nous avons évalué notre projet HumanForYou non seulement d’un point de vue technique, mais aussi sous l’angle de l’impact humain, organisationnel et sociétal.\n",
    "\n",
    "Les points clés sont :\n",
    "\n",
    "- Le modèle doit rester un **outil d’aide à la décision**, sous contrôle humain, jamais un mécanisme automatique de gestion de carrière.  \n",
    "- La **robustesse** et la **surveillance des biais** sont indispensables pour que le système soit fiable et équitable.  \n",
    "- La **confidentialité** et la **gouvernance des données** doivent être encadrées, avec une attention particulière portée à la transparence vis-à-vis des employés.  \n",
    "- L’objectif final est d’**améliorer les conditions de travail et la rétention**, en s’attaquant aux causes structurelles du turnover, et non simplement d’optimiser un indicateur chiffré.\n",
    "\n",
    "Ce livrable montre comment les exigences d’une IA digne de confiance peuvent être appliquées concrètement à un cas de prédiction du turnover et fournit une base pour des discussions plus approfondies avec la direction et les parties prenantes si le projet venait à dépasser le cadre pédagogique.\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc6e2e0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
